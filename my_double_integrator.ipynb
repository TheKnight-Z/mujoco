{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Define the toy double integrator environment ---\n",
    "class DoubleIntegratorEnv:\n",
    "    def __init__(self, dt=0.1, max_steps=200):\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.action_limit = 1.0  # clip acceleration to [-1, 1]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([0.0, 0.0])  # start at rest at the origin\n",
    "        self.step_count = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Clip action\n",
    "        action = np.clip(action, -self.action_limit, self.action_limit)\n",
    "        pos, vel = self.state\n",
    "        # Update dynamics: simple Euler integration\n",
    "        new_vel = vel + action * self.dt\n",
    "        new_pos = pos + vel * self.dt\n",
    "        self.state = np.array([new_pos, new_vel])\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Define a quadratic cost (reward is negative cost)\n",
    "        reward = - (new_pos**2 + new_vel**2)\n",
    "        done = self.step_count >= self.max_steps\n",
    "        return self.state.copy(), reward, done, {}\n",
    "\n",
    "# --- Define a simple PPO policy network ---\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        # Log std parameter for a Gaussian policy\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        mean = self.fc(state)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "# --- Define the PPO algorithm with reward logging ---\n",
    "class PPO:\n",
    "    def __init__(self, env, state_dim=2, action_dim=1, lr=3e-4, gamma=0.99, eps_clip=0.2, epochs=10):\n",
    "        self.env = env\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.epochs = epochs\n",
    "        # For logging total reward per episode\n",
    "        self.reward_log = []\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            states = []\n",
    "            actions = []\n",
    "            \n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action, log_prob = self.policy.sample_action(state_tensor)\n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            # Log total reward for this episode\n",
    "            total_reward = sum(rewards)\n",
    "            self.reward_log.append(total_reward)\n",
    "            \n",
    "            returns = self.compute_returns(rewards)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            states_tensor = torch.stack(states)\n",
    "            actions_tensor = torch.stack(actions)\n",
    "            old_log_probs = torch.stack(log_probs).squeeze()\n",
    "\n",
    "            # PPO policy update\n",
    "            for _ in range(self.epochs):\n",
    "                mean, std = self.policy(states_tensor)\n",
    "                dist = Normal(mean, std)\n",
    "                new_log_probs = dist.log_prob(actions_tensor).squeeze()\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "                advantages = returns\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "                loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode}: Loss = {loss.item():.4f}, Total Reward = {total_reward:.2f}\")\n",
    "\n",
    "        # After training, plot the reward curve\n",
    "        plt.plot(self.reward_log)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Reward Curve during Training')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# --- Main: Train the PPO agent on the double integrator ---\n",
    "if __name__ == \"__main__\":\n",
    "    # In an MJX setting, you would load your MuJoCo model and use mjx.put_model/data.\n",
    "    env = DoubleIntegratorEnv()\n",
    "    ppo_agent = PPO(env)\n",
    "    ppo_agent.train(num_episodes=1000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
